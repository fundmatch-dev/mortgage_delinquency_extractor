{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage-Backed Securities PDF Extraction: Approach Comparison\n",
    "\n",
    "This notebook compares three extraction approaches for extracting structured data from MBS trustee report PDFs:\n",
    "\n",
    "1. **Approach 1**: Gemini Vision Direct Extraction\n",
    "2. **Approach 2**: PDF Text Parsing + Gemini Pro\n",
    "3. **Approach 3**: Hybrid Smart Extraction\n",
    "\n",
    "We'll evaluate each approach on:\n",
    "- Accuracy (successful extraction rate, field-level accuracy)\n",
    "- Speed (time per document)\n",
    "- Cost (API token usage and estimated cost)\n",
    "- Quality (table handling, multi-page support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "# Import our extraction modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from utils.gemini_vision import GeminiVisionExtractor\n",
    "from utils.pdf_text_extraction import PDFTextExtractor\n",
    "from utils.validators import DataValidator\n",
    "from utils.output_formatter import ExcelFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")  # Or set directly: API_KEY = \"your-api-key\"\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Please set GOOGLE_API_KEY environment variable or set API_KEY directly\")\n",
    "\n",
    "# Paths\n",
    "PDF_DIR = Path(\"data/pdfs\")\n",
    "OUTPUT_DIR = Path(\"data/output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get sample PDFs (5-10 for comparison)\n",
    "all_pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "SAMPLE_SIZE = min(10, len(all_pdfs))\n",
    "sample_pdfs = all_pdfs[:SAMPLE_SIZE]\n",
    "\n",
    "print(f\"Total PDFs available: {len(all_pdfs)}\")\n",
    "print(f\"Sample size for comparison: {SAMPLE_SIZE}\")\n",
    "print(f\"Sample PDFs: {[p.name for p in sample_pdfs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Extraction Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Gemini Vision Direct Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractors\n",
    "vision_extractor = GeminiVisionExtractor(api_key=API_KEY, model_name=\"gemini-2.0-flash-exp\")\n",
    "validator = DataValidator()\n",
    "\n",
    "# Run Approach 1\n",
    "approach1_results = []\n",
    "\n",
    "print(\"Running Approach 1: Gemini Vision Direct Extraction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pdf_path in tqdm(sample_pdfs, desc=\"Approach 1\"):\n",
    "    result = vision_extractor.extract(pdf_path)\n",
    "    \n",
    "    # Add validation\n",
    "    if result[\"success\"]:\n",
    "        is_valid, validated_doc = validator.validate_document(result[\"data\"], pdf_path.name)\n",
    "        result[\"validation\"] = validator.get_validation_summary()\n",
    "        result[\"validation_passed\"] = is_valid\n",
    "    else:\n",
    "        result[\"validation_passed\"] = False\n",
    "    \n",
    "    approach1_results.append(result)\n",
    "    \n",
    "    # Print progress\n",
    "    status = \"OK\" if result[\"success\"] else f\"FAIL: {result['error'][:50]}\"\n",
    "    print(f\"  {pdf_path.name}: {status} ({result['processing_time']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: PDF Text Parsing + Gemini Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text extractor\n",
    "text_extractor = PDFTextExtractor(api_key=API_KEY, model_name=\"gemini-1.5-pro\")\n",
    "\n",
    "# Run Approach 2\n",
    "approach2_results = []\n",
    "\n",
    "print(\"Running Approach 2: PDF Text Parsing + Gemini Pro\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pdf_path in tqdm(sample_pdfs, desc=\"Approach 2\"):\n",
    "    result = text_extractor.extract(pdf_path, text_method=\"pdfplumber\")\n",
    "    \n",
    "    # Add validation\n",
    "    if result[\"success\"]:\n",
    "        is_valid, validated_doc = validator.validate_document(result[\"data\"], pdf_path.name)\n",
    "        result[\"validation\"] = validator.get_validation_summary()\n",
    "        result[\"validation_passed\"] = is_valid\n",
    "    else:\n",
    "        result[\"validation_passed\"] = False\n",
    "    \n",
    "    approach2_results.append(result)\n",
    "    \n",
    "    status = \"OK\" if result[\"success\"] else f\"FAIL: {result['error'][:50]}\"\n",
    "    print(f\"  {pdf_path.name}: {status} ({result['processing_time']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Hybrid Smart Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_text_quality(text: str, metadata: dict) -> bool:\n",
    "    \"\"\"Assess if extracted text is good enough for text-based extraction.\"\"\"\n",
    "    if not text or len(text) < 500:\n",
    "        return False\n",
    "    \n",
    "    has_tables = metadata.get(\"tables_found\", 0) > 0\n",
    "    delinquency_keywords = [\"current\", \"delinquent\", \"30-59\", \"60-89\", \"90+\", \"foreclosure\"]\n",
    "    keyword_count = sum(1 for kw in delinquency_keywords if kw.lower() in text.lower())\n",
    "    has_balance_data = \"$\" in text or \"balance\" in text.lower()\n",
    "    \n",
    "    quality_score = 0\n",
    "    if has_tables:\n",
    "        quality_score += 2\n",
    "    if keyword_count >= 3:\n",
    "        quality_score += 2\n",
    "    if has_balance_data:\n",
    "        quality_score += 1\n",
    "    if len(text) > 2000:\n",
    "        quality_score += 1\n",
    "    \n",
    "    return quality_score >= 4\n",
    "\n",
    "# Initialize extractors for hybrid approach\n",
    "hybrid_text_extractor = PDFTextExtractor(api_key=API_KEY, model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "# Run Approach 3\n",
    "approach3_results = []\n",
    "\n",
    "print(\"Running Approach 3: Hybrid Smart Extraction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pdf_path in tqdm(sample_pdfs, desc=\"Approach 3\"):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Assess text quality first\n",
    "    try:\n",
    "        text, text_metadata = hybrid_text_extractor.extract_text(pdf_path, method=\"pdfplumber\")\n",
    "        use_text_method = assess_text_quality(text, text_metadata)\n",
    "    except Exception:\n",
    "        use_text_method = False\n",
    "    \n",
    "    # Choose method based on quality\n",
    "    if use_text_method:\n",
    "        result = hybrid_text_extractor.extract(pdf_path, text_method=\"pdfplumber\")\n",
    "        result[\"hybrid_method\"] = \"text\"\n",
    "    else:\n",
    "        result = vision_extractor.extract(pdf_path)\n",
    "        result[\"hybrid_method\"] = \"vision\"\n",
    "    \n",
    "    # Add validation\n",
    "    if result[\"success\"]:\n",
    "        is_valid, validated_doc = validator.validate_document(result[\"data\"], pdf_path.name)\n",
    "        result[\"validation\"] = validator.get_validation_summary()\n",
    "        result[\"validation_passed\"] = is_valid\n",
    "    else:\n",
    "        result[\"validation_passed\"] = False\n",
    "    \n",
    "    approach3_results.append(result)\n",
    "    \n",
    "    method = result.get(\"hybrid_method\", \"unknown\")\n",
    "    status = \"OK\" if result[\"success\"] else f\"FAIL\"\n",
    "    print(f\"  {pdf_path.name}: {status} [{method}] ({result['processing_time']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantitative Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results, approach_name):\n",
    "    \"\"\"Calculate metrics for a set of results.\"\"\"\n",
    "    total = len(results)\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    validated = sum(1 for r in results if r.get(\"validation_passed\", False))\n",
    "    \n",
    "    # Calculate field-level accuracy for successful extractions\n",
    "    field_scores = {\"series_name\": 0, \"report_date\": 0, \"beginning_balance\": 0, \n",
    "                   \"ending_balance\": 0, \"delinquency\": 0}\n",
    "    \n",
    "    for r in results:\n",
    "        if r[\"success\"] and r.get(\"data\"):\n",
    "            data = r[\"data\"]\n",
    "            if data.get(\"series_name\"):\n",
    "                field_scores[\"series_name\"] += 1\n",
    "            if data.get(\"report_date\"):\n",
    "                field_scores[\"report_date\"] += 1\n",
    "            if data.get(\"beginning_balance\") is not None:\n",
    "                field_scores[\"beginning_balance\"] += 1\n",
    "            if data.get(\"ending_balance\") is not None:\n",
    "                field_scores[\"ending_balance\"] += 1\n",
    "            if data.get(\"delinquency\") and len(data.get(\"delinquency\", [])) > 0:\n",
    "                field_scores[\"delinquency\"] += 1\n",
    "    \n",
    "    # Calculate processing times\n",
    "    times = [r[\"processing_time\"] for r in results]\n",
    "    \n",
    "    return {\n",
    "        \"approach\": approach_name,\n",
    "        \"total_documents\": total,\n",
    "        \"successful_extractions\": successful,\n",
    "        \"success_rate\": successful / total * 100 if total > 0 else 0,\n",
    "        \"validation_passed\": validated,\n",
    "        \"validation_rate\": validated / total * 100 if total > 0 else 0,\n",
    "        \"field_accuracy\": {k: v / successful * 100 if successful > 0 else 0 for k, v in field_scores.items()},\n",
    "        \"avg_time\": np.mean(times),\n",
    "        \"min_time\": np.min(times),\n",
    "        \"max_time\": np.max(times),\n",
    "        \"total_time\": np.sum(times),\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each approach\n",
    "metrics_1 = calculate_metrics(approach1_results, \"Approach 1: Gemini Vision\")\n",
    "metrics_2 = calculate_metrics(approach2_results, \"Approach 2: Text + Gemini Pro\")\n",
    "metrics_3 = calculate_metrics(approach3_results, \"Approach 3: Hybrid\")\n",
    "\n",
    "all_metrics = [metrics_1, metrics_2, metrics_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for m in all_metrics:\n",
    "    comparison_data.append({\n",
    "        \"Approach\": m[\"approach\"],\n",
    "        \"Success Rate (%)\": f\"{m['success_rate']:.1f}\",\n",
    "        \"Validation Rate (%)\": f\"{m['validation_rate']:.1f}\",\n",
    "        \"Avg Time (s)\": f\"{m['avg_time']:.2f}\",\n",
    "        \"Total Time (s)\": f\"{m['total_time']:.2f}\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ACCURACY AND SPEED COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field-level accuracy comparison\n",
    "field_data = []\n",
    "for m in all_metrics:\n",
    "    for field, accuracy in m[\"field_accuracy\"].items():\n",
    "        field_data.append({\n",
    "            \"Approach\": m[\"approach\"].split(\":\")[0],\n",
    "            \"Field\": field,\n",
    "            \"Accuracy (%)\": accuracy\n",
    "        })\n",
    "\n",
    "field_df = pd.DataFrame(field_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIELD-LEVEL ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pivot for better display\n",
    "field_pivot = field_df.pivot(index=\"Field\", columns=\"Approach\", values=\"Accuracy (%)\")\n",
    "display(field_pivot.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(results, approach_name):\n",
    "    \"\"\"Analyze errors for a set of results.\"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    for r in results:\n",
    "        if not r[\"success\"]:\n",
    "            errors.append({\"file\": r[\"filename\"], \"error\": r[\"error\"]})\n",
    "        elif r.get(\"validation\"):\n",
    "            if r[\"validation\"].get(\"errors\"):\n",
    "                for e in r[\"validation\"][\"errors\"]:\n",
    "                    errors.append({\"file\": r[\"filename\"], \"error\": e})\n",
    "            if r[\"validation\"].get(\"warnings\"):\n",
    "                for w in r[\"validation\"][\"warnings\"]:\n",
    "                    warnings.append({\"file\": r[\"filename\"], \"warning\": w})\n",
    "    \n",
    "    return {\"approach\": approach_name, \"errors\": errors, \"warnings\": warnings}\n",
    "\n",
    "errors_1 = analyze_errors(approach1_results, \"Approach 1\")\n",
    "errors_2 = analyze_errors(approach2_results, \"Approach 2\")\n",
    "errors_3 = analyze_errors(approach3_results, \"Approach 3\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for e in [errors_1, errors_2, errors_3]:\n",
    "    print(f\"\\n{e['approach']}:\")\n",
    "    print(f\"  Errors: {len(e['errors'])}\")\n",
    "    print(f\"  Warnings: {len(e['warnings'])}\")\n",
    "    if e['errors']:\n",
    "        print(\"  Error details:\")\n",
    "        for err in e['errors'][:5]:  # Show first 5\n",
    "            print(f\"    - {err['file']}: {err['error'][:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate costs (rough approximations)\n",
    "NUM_TOTAL_DOCS = 50\n",
    "AVG_PAGES_PER_DOC = 10  # Estimate - adjust based on actual PDFs\n",
    "AVG_TEXT_LENGTH = 15000  # Estimate - adjust based on actual extraction\n",
    "\n",
    "cost_1 = vision_extractor.estimate_cost(AVG_PAGES_PER_DOC, NUM_TOTAL_DOCS)\n",
    "cost_2 = text_extractor.estimate_cost(AVG_TEXT_LENGTH, NUM_TOTAL_DOCS)\n",
    "\n",
    "# Hybrid cost estimation (assume 60% text, 40% vision based on quality assessment)\n",
    "hybrid_text_ratio = 0.6\n",
    "cost_3 = {\n",
    "    \"estimated_total_cost\": (\n",
    "        cost_2[\"estimated_total_cost\"] * hybrid_text_ratio + \n",
    "        cost_1[\"estimated_total_cost\"] * (1 - hybrid_text_ratio)\n",
    "    ),\n",
    "    \"cost_per_document\": (\n",
    "        cost_2[\"cost_per_document\"] * hybrid_text_ratio + \n",
    "        cost_1[\"cost_per_document\"] * (1 - hybrid_text_ratio)\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COST ESTIMATION (for 50 documents)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cost_data = [\n",
    "    {\"Approach\": \"Approach 1: Gemini Vision\", \n",
    "     \"Est. Total Cost\": f\"${cost_1['estimated_total_cost']:.4f}\",\n",
    "     \"Cost per Doc\": f\"${cost_1['cost_per_document']:.6f}\"},\n",
    "    {\"Approach\": \"Approach 2: Text + Pro\", \n",
    "     \"Est. Total Cost\": f\"${cost_2['estimated_total_cost']:.4f}\",\n",
    "     \"Cost per Doc\": f\"${cost_2['cost_per_document']:.6f}\"},\n",
    "    {\"Approach\": \"Approach 3: Hybrid\", \n",
    "     \"Est. Total Cost\": f\"${cost_3['estimated_total_cost']:.4f}\",\n",
    "     \"Cost per Doc\": f\"${cost_3['cost_per_document']:.6f}\"},\n",
    "]\n",
    "\n",
    "cost_df = pd.DataFrame(cost_data)\n",
    "display(cost_df)\n",
    "\n",
    "print(\"\\nNote: These are rough estimates. Actual costs may vary based on document complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization data\n",
    "approaches = [\"Approach 1\\n(Vision)\", \"Approach 2\\n(Text+Pro)\", \"Approach 3\\n(Hybrid)\"]\n",
    "success_rates = [metrics_1[\"success_rate\"], metrics_2[\"success_rate\"], metrics_3[\"success_rate\"]]\n",
    "validation_rates = [metrics_1[\"validation_rate\"], metrics_2[\"validation_rate\"], metrics_3[\"validation_rate\"]]\n",
    "avg_times = [metrics_1[\"avg_time\"], metrics_2[\"avg_time\"], metrics_3[\"avg_time\"]]\n",
    "costs = [cost_1[\"cost_per_document\"] * 1000, cost_2[\"cost_per_document\"] * 1000, cost_3[\"cost_per_document\"] * 1000]  # in millicents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Success and Validation Rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Success Rate\n",
    "colors = sns.color_palette(\"husl\", 3)\n",
    "bars1 = axes[0].bar(approaches, success_rates, color=colors)\n",
    "axes[0].set_ylabel(\"Success Rate (%)\")\n",
    "axes[0].set_title(\"Extraction Success Rate\")\n",
    "axes[0].set_ylim(0, 105)\n",
    "for bar, rate in zip(bars1, success_rates):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{rate:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Processing Time\n",
    "bars2 = axes[1].bar(approaches, avg_times, color=colors)\n",
    "axes[1].set_ylabel(\"Average Time (seconds)\")\n",
    "axes[1].set_title(\"Average Processing Time per Document\")\n",
    "for bar, t in zip(bars2, avg_times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                f'{t:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Cost\n",
    "bars3 = axes[2].bar(approaches, costs, color=colors)\n",
    "axes[2].set_ylabel(\"Cost per Document (millicents)\")\n",
    "axes[2].set_title(\"Estimated Cost per Document\")\n",
    "for bar, c in zip(bars3, costs):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                f'{c:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"comparison_charts.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Field-level Accuracy Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "field_matrix = field_pivot.values\n",
    "sns.heatmap(field_pivot, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            vmin=0, vmax=100, ax=ax, cbar_kws={'label': 'Accuracy (%)'})\n",
    "ax.set_title(\"Field-Level Extraction Accuracy by Approach\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Field\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"field_accuracy_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Processing Time Distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "time_data = {\n",
    "    \"Approach 1 (Vision)\": [r[\"processing_time\"] for r in approach1_results],\n",
    "    \"Approach 2 (Text+Pro)\": [r[\"processing_time\"] for r in approach2_results],\n",
    "    \"Approach 3 (Hybrid)\": [r[\"processing_time\"] for r in approach3_results],\n",
    "}\n",
    "\n",
    "time_df = pd.DataFrame(time_data)\n",
    "time_df.boxplot(ax=ax)\n",
    "ax.set_ylabel(\"Processing Time (seconds)\")\n",
    "ax.set_title(\"Processing Time Distribution by Approach\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"time_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text extraction quality for Approach 2\n",
    "print(\"=\" * 80)\n",
    "print(\"TEXT EXTRACTION QUALITY ANALYSIS (Approach 2)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for r in approach2_results:\n",
    "    if r.get(\"text_extraction_metadata\"):\n",
    "        meta = r[\"text_extraction_metadata\"]\n",
    "        print(f\"\\n{r['filename']}:\")\n",
    "        print(f\"  Pages: {meta.get('pages', 'N/A')}\")\n",
    "        print(f\"  Tables found: {meta.get('tables_found', 'N/A')}\")\n",
    "        print(f\"  Method: {meta.get('method', 'N/A')}\")\n",
    "        print(f\"  Extraction success: {r['success']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hybrid method distribution\n",
    "if approach3_results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HYBRID APPROACH METHOD DISTRIBUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_count = sum(1 for r in approach3_results if r.get(\"hybrid_method\") == \"text\")\n",
    "    vision_count = sum(1 for r in approach3_results if r.get(\"hybrid_method\") == \"vision\")\n",
    "    \n",
    "    print(f\"\\nText-based extraction: {text_count} ({text_count/len(approach3_results)*100:.1f}%)\")\n",
    "    print(f\"Vision-based extraction: {vision_count} ({vision_count/len(approach3_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Create pie chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.pie([text_count, vision_count], labels=['Text Method', 'Vision Method'], \n",
    "           autopct='%1.1f%%', colors=sns.color_palette(\"husl\", 2))\n",
    "    ax.set_title(\"Approach 3: Method Distribution\")\n",
    "    plt.savefig(OUTPUT_DIR / \"hybrid_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample extracted data from each approach\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE EXTRACTED DATA COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get first successful extraction from each approach\n",
    "for approach_name, results in [(\"Approach 1\", approach1_results), \n",
    "                                (\"Approach 2\", approach2_results),\n",
    "                                (\"Approach 3\", approach3_results)]:\n",
    "    print(f\"\\n{approach_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    successful = [r for r in results if r[\"success\"]]\n",
    "    if successful:\n",
    "        sample = successful[0]\n",
    "        data = sample[\"data\"]\n",
    "        print(f\"  Filename: {data.get('filename')}\")\n",
    "        print(f\"  Series: {data.get('series_name')}\")\n",
    "        print(f\"  Date: {data.get('report_date')}\")\n",
    "        print(f\"  Beginning Balance: ${data.get('beginning_balance', 0):,.2f}\")\n",
    "        print(f\"  Ending Balance: ${data.get('ending_balance', 0):,.2f}\")\n",
    "        print(f\"  Delinquency Categories: {len(data.get('delinquency', []))}\")\n",
    "        if data.get('delinquency'):\n",
    "            for d in data['delinquency'][:3]:\n",
    "                print(f\"    - {d.get('category')}: {d.get('count')} loans, ${d.get('balance', 0):,.2f}\")\n",
    "    else:\n",
    "        print(\"  No successful extractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Score each approach (simple weighted scoring)\n",
    "scores = {}\n",
    "for m in all_metrics:\n",
    "    # Weights: accuracy 40%, speed 30%, cost 30%\n",
    "    accuracy_score = m[\"success_rate\"] * 0.4\n",
    "    \n",
    "    # Normalize speed (lower is better, max 100)\n",
    "    max_time = max(metrics_1[\"avg_time\"], metrics_2[\"avg_time\"], metrics_3[\"avg_time\"])\n",
    "    speed_score = (1 - m[\"avg_time\"] / max_time) * 100 * 0.3 if max_time > 0 else 30\n",
    "    \n",
    "    # Normalize cost (lower is better, max 100)\n",
    "    approach_num = int(m[\"approach\"].split()[1][0])\n",
    "    cost_val = [cost_1, cost_2, cost_3][approach_num - 1][\"cost_per_document\"]\n",
    "    max_cost = max(cost_1[\"cost_per_document\"], cost_2[\"cost_per_document\"], cost_3[\"cost_per_document\"])\n",
    "    cost_score = (1 - cost_val / max_cost) * 100 * 0.3 if max_cost > 0 else 30\n",
    "    \n",
    "    total_score = accuracy_score + speed_score + cost_score\n",
    "    scores[m[\"approach\"]] = {\n",
    "        \"total\": total_score,\n",
    "        \"accuracy_component\": accuracy_score,\n",
    "        \"speed_component\": speed_score,\n",
    "        \"cost_component\": cost_score,\n",
    "    }\n",
    "\n",
    "# Find best approach\n",
    "best_approach = max(scores.items(), key=lambda x: x[1][\"total\"])\n",
    "\n",
    "print(\"\\nWeighted Scoring (Accuracy: 40%, Speed: 30%, Cost: 30%):\")\n",
    "print(\"-\" * 60)\n",
    "for approach, score in sorted(scores.items(), key=lambda x: -x[1][\"total\"]):\n",
    "    print(f\"\\n{approach}:\")\n",
    "    print(f\"  Total Score: {score['total']:.1f}\")\n",
    "    print(f\"  - Accuracy: {score['accuracy_component']:.1f}\")\n",
    "    print(f\"  - Speed: {score['speed_component']:.1f}\")\n",
    "    print(f\"  - Cost: {score['cost_component']:.1f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"RECOMMENDED: {best_approach[0]}\")\n",
    "print(f\"Total Score: {best_approach[1]['total']:.1f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate qualitative recommendation summary\n",
    "print(\"\\nQUALITATIVE ANALYSIS SUMMARY:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "Approach 1 (Gemini Vision):\n",
    "  - Pros: Best for complex table structures, handles scanned PDFs\n",
    "  - Cons: Higher cost, slower processing\n",
    "  - Best for: Documents with complex layouts or poor text extraction\n",
    "\n",
    "Approach 2 (Text + Gemini Pro):\n",
    "  - Pros: Lower cost, faster when text extraction works well\n",
    "  - Cons: May struggle with complex tables, depends on PDF quality\n",
    "  - Best for: Clean, text-based PDFs with simple layouts\n",
    "\n",
    "Approach 3 (Hybrid):\n",
    "  - Pros: Balances cost and accuracy, adapts to document quality\n",
    "  - Cons: Additional complexity, two-step assessment\n",
    "  - Best for: Mixed document quality in batch processing\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nFINAL RECOMMENDATION FOR FULL EXTRACTION:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Based on this analysis, use {best_approach[0]} for processing all 50 documents.\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(\"- Review the success rates and field accuracy above\")\n",
    "print(\"- Consider the cost-accuracy tradeoff for your specific use case\")\n",
    "print(\"- If accuracy is paramount, prioritize Approach 1 (Vision)\")\n",
    "print(\"- If cost efficiency matters more, consider Approach 2 or 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to JSON\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "all_results = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"sample_size\": SAMPLE_SIZE,\n",
    "    \"approach_1_results\": approach1_results,\n",
    "    \"approach_2_results\": approach2_results,\n",
    "    \"approach_3_results\": approach3_results,\n",
    "    \"metrics\": {\n",
    "        \"approach_1\": metrics_1,\n",
    "        \"approach_2\": metrics_2,\n",
    "        \"approach_3\": metrics_3,\n",
    "    },\n",
    "    \"recommendation\": best_approach[0],\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / f\"comparison_results_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {OUTPUT_DIR / f'comparison_results_{timestamp}.json'}\")\n",
    "print(f\"Charts saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
